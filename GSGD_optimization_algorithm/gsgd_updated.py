# -*- coding: utf-8 -*-
"""GSGD_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mzv-M3v5TPBEOuAJr1s7_UZpEittCLUC

Contents of main.py
"""

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from model import CNN_GSGD, GSGDOptimizer
from train import train, test
import os
import torch.nn as nn
from torch.utils.data import random_split

"""# Data loading, model setup, and main training loop code here"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

"""HISTOGRAM"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to the ZIP file
zip_file_path = '/content/drive/My Drive/Soft Computing Resources/AlgX3_64x64_merged_tensors_with_labels.zip'
extract_path = '/content/'

# Verify if the ZIP file exists
if not os.path.exists(zip_file_path):
    raise FileNotFoundError(f"File not found: {zip_file_path}")

# Copy the ZIP file to Colab
!cp "{zip_file_path}" /content

# Unzip the file
with zipfile.ZipFile('/content/AlgX3_64x64_merged_tensors_with_labels.zip', 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the path to the folder where your images are stored
data_path = '/content/AlgX3_64x64_merged_tensors_with_labels'  # Update this with the path to your images

# Step 1: Set up transformations (similar to MNIST preprocessing)
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed
    # transforms.Resize((28, 28)),  # Resize to 28x28 to match MNIST dimensions, adjust if needed
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize to match MNIST-like preprocessing
])

# Step 2: Load the entire dataset using ImageFolder
full_dataset = datasets.ImageFolder(root=data_path, transform=transform)

# Step 3: Get input size dynamically from the first image in the dataset
sample_image, _ = full_dataset[0]  # Take the first image to determine the input shape
input_size = sample_image.shape  # This will be (C, H, W)

# Step 4: Split the dataset into training and testing sets
train_size = int(0.8 * len(full_dataset))  # 80% for training
test_size = len(full_dataset) - train_size  # Remaining 20% for testing
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

# Step 5: Create DataLoaders for both sets
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Split the train_dataset into training and validation datasets
train_size = int(0.8 * len(train_dataset))  # 80% for training
validation_size = len(train_dataset) - train_size  # Remaining 20% for validation
train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_GSGD(input_size=input_size, num_classes=len(full_dataset.classes)).to(device)

# model = EnhancedCNN_GSGD(num_classes=len(full_dataset.classes)).to(device)


# For Standard Guided SGD
# optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='sgd')

# For Guided SGD with Momentum
optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='momentum', momentum=0.9)

# For Guided Adam
# optimizer = GSGDOptimizer(model.parameters(), lr=0.001, method='adam', beta1=0.9, beta2=0.999)

# Define the loss function
loss_fn = nn.CrossEntropyLoss()

# Run training for each epoch, passing the dataset instead of DataLoader
for epoch in range(1, 30):
    train(model, device, train_loader.dataset, validation_loader.dataset, optimizer, epoch, loss_fn,
      verification_set_num=4, rho=10, log_interval=9)
    # train(model, device, train_loader.dataset, optimizer, epoch, loss_fn)  # Pass train_loader.dataset
    test(model, device, test_loader)

"""BAR GRAPHS"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to the ZIP file
zip_file_path = '/content/drive/My Drive/Heatmaps/bargraphs.zip'
extract_path = '/content/bargraphs'

# Verify if the ZIP file exists
if not os.path.exists(zip_file_path):
    raise FileNotFoundError(f"File not found: {zip_file_path}")

# Copy the ZIP file to Colab
!cp "{zip_file_path}" /content

# Unzip the file
with zipfile.ZipFile('/content/bargraphs.zip', 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the path to the folder where your images are stored
data_path = '/content/bargraphs'  # Update this with the path to your images

# Step 1: Set up transformations (similar to MNIST preprocessing)
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed
    # transforms.Resize((28, 28)),  # Resize to 28x28 to match MNIST dimensions, adjust if needed
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize to match MNIST-like preprocessing
])

# Step 2: Load the entire dataset using ImageFolder
full_dataset = datasets.ImageFolder(root=data_path, transform=transform)

# Step 3: Get input size dynamically from the first image in the dataset
sample_image, _ = full_dataset[0]  # Take the first image to determine the input shape
input_size = sample_image.shape  # This will be (C, H, W)

# Step 4: Split the dataset into training and testing sets
train_size = int(0.8 * len(full_dataset))  # 80% for training
test_size = len(full_dataset) - train_size  # Remaining 20% for testing
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

# Step 5: Create DataLoaders for both sets
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Split the train_dataset into training and validation datasets
train_size = int(0.8 * len(train_dataset))  # 80% for training
validation_size = len(train_dataset) - train_size  # Remaining 20% for validation
train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_GSGD(input_size=input_size, num_classes=len(full_dataset.classes)).to(device)

# model = EnhancedCNN_GSGD(num_classes=len(full_dataset.classes)).to(device)


# For Standard Guided SGD
# optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='sgd')

# For Guided SGD with Momentum
optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='momentum', momentum=0.9)

# For Guided Adam
# optimizer = GSGDOptimizer(model.parameters(), lr=0.001, method='adam', beta1=0.9, beta2=0.999)


# Define the loss function
loss_fn = nn.CrossEntropyLoss()

# Run training for each epoch, passing the dataset instead of DataLoader
for epoch in range(1, 30):
    train(model, device, train_loader.dataset, validation_loader.dataset, optimizer, epoch, loss_fn,
      verification_set_num=4, rho=10, log_interval=9)
    # train(model, device, train_loader.dataset, optimizer, epoch, loss_fn)  # Pass train_loader.dataset
    test(model, device, test_loader)

"""HEATMAPS"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to the ZIP file
zip_file_path = '/content/drive/My Drive/Heatmaps/heatmaps.zip'
extract_path = '/content/heatmaps'

# Verify if the ZIP file exists
if not os.path.exists(zip_file_path):
    raise FileNotFoundError(f"File not found: {zip_file_path}")

# Copy the ZIP file to Colab
!cp "{zip_file_path}" /content

# Unzip the file
with zipfile.ZipFile('/content/heatmaps.zip', 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the path to the folder where your images are stored
data_path = '/content/heatmaps'  # Update this with the path to your images

# Step 1: Set up transformations (similar to MNIST preprocessing)
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed
    # transforms.Resize((28, 28)),  # Resize to 28x28 to match MNIST dimensions, adjust if needed
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize to match MNIST-like preprocessing
])

# Step 2: Load the entire dataset using ImageFolder
full_dataset = datasets.ImageFolder(root=data_path, transform=transform)

# Step 3: Get input size dynamically from the first image in the dataset
sample_image, _ = full_dataset[0]  # Take the first image to determine the input shape
input_size = sample_image.shape  # This will be (C, H, W)

# Step 4: Split the dataset into training and testing sets
train_size = int(0.8 * len(full_dataset))  # 80% for training
test_size = len(full_dataset) - train_size  # Remaining 20% for testing
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

# Step 5: Create DataLoaders for both sets
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Split the train_dataset into training and validation datasets
train_size = int(0.8 * len(train_dataset))  # 80% for training
validation_size = len(train_dataset) - train_size  # Remaining 20% for validation
train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_GSGD(input_size=input_size, num_classes=len(full_dataset.classes)).to(device)

# model = EnhancedCNN_GSGD(num_classes=len(full_dataset.classes)).to(device)
# For Standard Guided SGD
# optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='sgd')

# For Guided SGD with Momentum
optimizer = GSGDOptimizer(model.parameters(), lr=0.01, method='momentum', momentum=0.9)

# For Guided Adam
# optimizer = GSGDOptimizer(model.parameters(), lr=0.001, method='adam', beta1=0.9, beta2=0.999)

# Define the loss function
loss_fn = nn.CrossEntropyLoss()

# Run training for each epoch, passing the dataset instead of DataLoader
for epoch in range(1, 30):
    train(model, device, train_loader.dataset, validation_loader.dataset, optimizer, epoch, loss_fn,
      verification_set_num=4, rho=10, log_interval=2)
    # train(model, device, train_loader.dataset, optimizer, epoch, loss_fn)  # Pass train_loader.dataset
    test(model, device, test_loader)
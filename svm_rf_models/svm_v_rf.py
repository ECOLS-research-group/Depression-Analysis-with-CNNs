# -*- coding: utf-8 -*-
"""SVM v RF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gN7kPjJ0SmJ88X1FbEDuey5Xdbu5r2X0
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to your file on Google Drive
file_path = "/content/drive/My Drive/merged_tensors_with_labels.csv"

# Copy the file to Colab under the /content directory
!cp "{file_path}" /content

# Load the CSV file into a DataFrame
import pandas as pd
df = pd.read_csv("/content/merged_tensors_with_labels.csv")

# Display the DataFrame
print(df.head())

"""RANDOM FOREST

1. Data Preparation
"""

# Separate features and target label
X = df.drop(columns=['is_depression'])  # Features (768 columns)
y = df['is_depression']  # Target (depression label)

# Split the dataset into training and testing sets (80% train, 20% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""2. Model Initialization and Training"""

# Initialize the Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(X_train, y_train)

"""3. Predictions and Evaluation"""

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the classifier's performance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_rep)
print("\nConfusion Matrix:\n", confusion_mat)

"""4. Feature Importances"""

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame to view the importances
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Display the top 10 features
print("\nTop 10 Important Features:\n", feature_importance_df.head(10))

"""5. Cross-Validation"""

# Cross-validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(rf_classifier, X, y, cv=5)  # 5-fold cross-validation
print(f"\nCross-Validation Scores: {cv_scores}")
print(f"Mean CV Score: {cv_scores.mean():.2f}")

"""6. ROC Curve and AUC"""

# ROC Curve and AUC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_probs = rf_classifier.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc_score = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

"""7. Hyperparameter Tuning"""

# Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

"""8. Misclassified Instances"""

# Identify misclassified instances
misclassified = X_test[y_test != y_pred]
print("Misclassified Instances:\n", misclassified)

"""9. Model Saving"""

# Save the model
import joblib
joblib.dump(rf_classifier, '/content/random_forest_model.pkl')

# Load the model later (uncomment the next line to load)
# loaded_model = joblib.load('/content/random_forest_model.pkl')

"""10. SHAP Values for Model Interpretability"""

# SHAP Values for model interpretability
import shap

# Create SHAP explainer
explainer = shap.TreeExplainer(rf_classifier)
shap_values = explainer.shap_values(X_test)

# Plot SHAP summary
shap.initjs()
shap.summary_plot(shap_values, X_test)

"""Support Vector Machine (SVM)

1. Data Preparation
"""

# Separate features and target label
X = df.drop(columns=['is_depression'])  # Features (768 columns)
y = df['is_depression']  # Target (depression label)

# Split the dataset into training and testing sets (80% train, 20% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""2. Model Initialization and Training"""

# Initialize the Support Vector Classifier
from sklearn.svm import SVC
svm_classifier = SVC(probability=True, random_state=42)  # probability=True for ROC curve

# Train the classifier
svm_classifier.fit(X_train, y_train)

"""3. Predictions and Evaluation"""

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test)

# Evaluate the classifier's performance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_rep)
print("\nConfusion Matrix:\n", confusion_mat)

"""4. Cross-Validation"""

# Cross-validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(svm_classifier, X, y, cv=5)  # 5-fold cross-validation
print(f"\nCross-Validation Scores: {cv_scores}")
print(f"Mean CV Score: {cv_scores.mean():.2f}")

"""5. ROC Curve and AUC"""

# ROC Curve and AUC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_probs = svm_classifier.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc_score = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

"""6. Hyperparameter Tuning"""

# Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1],
    'kernel': ['linear', 'rbf', 'poly']
}

grid_search = GridSearchCV(SVC(probability=True, random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

"""7. Misclassified Instances"""

# Identify misclassified instances
misclassified = X_test[y_test != y_pred]
print("Misclassified Instances:\n", misclassified)

"""8. Model Saving"""

# Save the model
import joblib
joblib.dump(svm_classifier, '/content/svm_model.pkl')

# Load the model later (uncomment the next line to load)
# loaded_model = joblib.load('/content/svm_model.pkl')

"""9. SHAP Values for Model Interpretability"""

# SHAP Values for model interpretability
import shap

# Create SHAP explainer
explainer = shap.KernelExplainer(svm_classifier.predict_proba, X_train)  # Using KernelExplainer for SVM
shap_values = explainer.shap_values(X_test)

# Plot SHAP summary
shap.initjs()
shap.summary_plot(shap_values, X_test)
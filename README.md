# ğŸ§  Depression Analysis with CNNs

**Revealing Hidden Pain: A Comparative Study of Traditional, Hybrid, and Deep Learning Models for Depression Detection on Social Media**

---

## ğŸ“Œ Project Overview

This repository contains the complete implementation of the research work:

> **Revealing Hidden Pain: A Comparative Analysis of Traditional vs. New Deep Learning Approaches for Detecting Depression on Social Media**

The study investigates depression detection from social media text using:

* **Traditional machine learning models** (LSTM, SVM, RF, 1D CNN)
* **Hybrid models** (BERT combined with classical and neural classifiers)
* **Image-based deep learning models** (2D CNN, Vision Transformer)

A key contribution of this work is the **transformation of BERT embedding vectors into image representations** (heatmaps, bar graphs, and histograms), enabling the application of **2D CNNs and Vision Transformers** for depression classification.

---

## ğŸ‘¨â€ğŸ’» Authors & Roles

* **Anuraag Raj**
  *Primary Programmer & Research Developer*
  Designed and implemented all algorithms, data preprocessing pipelines, embedding-to-image transformations, model architectures, and experimental evaluations in Python.

* **Dr. Anuraganand Sharma**
  *Research Supervisor & Project Manager*
  Supervised the research methodology, guided experimental design, validated results, and managed the overall research project.

---

## ğŸ“„ Publication Information

**Paper Title:**
*Revealing Hidden Pain: A Comparative Analysis of Traditional vs. New Deep Learning Approaches for Detecting Depression on Social Media*

**Journal:**
IEEE Access

**Publication Link:**
https://ieeexplore.ieee.org/document/11366856

**Authors:**
Anuraag Raj, Anuraganand Sharma

---

## ğŸ“ Repository Structure

```
DEPRESSION-ANALYSIS-WITH-CNNS/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ depression_dataset_reddit_cleaned.csv
â”‚   â”œâ”€â”€ Mental-Health-Twitter.csv
â”‚   â”œâ”€â”€ merged_tensors_with_labels.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ AlgX3_64x64_merged_tensors_with_labels/
â”‚   â”‚   â”œâ”€â”€ 0/   (non-depressed class images)
â”‚   â”‚   â””â”€â”€ 1/   (depressed class images)
â”‚   â”‚
â”‚   â”œâ”€â”€ Bargraphs_merged_tensors_with_labels/
â”‚   â”‚   â”œâ”€â”€ 0/
â”‚   â”‚   â””â”€â”€ 1/
â”‚   â”‚
â”‚   â””â”€â”€ Heatmaps_merged_tensors_with_labels/
â”‚       â”œâ”€â”€ 0/
â”‚       â””â”€â”€ 1/
â”‚
â”œâ”€â”€ deep_learning_models/
â”‚   â”œâ”€â”€ 2dcnn_tweets.ipynb
â”‚   â”œâ”€â”€ bert_only.ipynb
â”‚   â”œâ”€â”€ bert_tweets.ipynb
â”‚   â”œâ”€â”€ BERT_v_Autoencoder.ipynb
â”‚   â”œâ”€â”€ embedding_to_image_mapping.ipynb
â”‚   â”œâ”€â”€ GSGD_CNN.ipynb
â”‚   â”œâ”€â”€ ViT.ipynb
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ traditional_vs_hybrid_models/
â”‚   â”œâ”€â”€ traditional_text_models.ipynb
â”‚   â”œâ”€â”€ lstm_tweets.ipynb
â”‚   â””â”€â”€ bert_hybrid_models.ipynb
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“Š Dataset Description

### Primary Dataset (Reddit)

* **`depression_dataset_reddit_cleaned.csv`**
  A cleaned and preprocessed dataset of Reddit posts labeled as depressed (1) or non-depressed (0).

* **`merged_tensors_with_labels.csv`**
  BERT-encoded sentence embeddings of Reddit posts along with class labels.

### Secondary Dataset (Twitter)

* **`Mental-Health-Twitter.csv`**
  A secondary dataset consisting of Twitter posts related to mental health.
  This dataset is used **to analyze trends and evaluate the generalization behavior of trained models**, ensuring that the learned patterns are not restricted to a single social media platform.

---

## ğŸ–¼ Image Representation (Key Innovation)

BERT embedding vectors are transformed into 2D image representations for CNN and Vision Transformer processing.

### Transformation Types

* **Heatmaps** â€“ visualize embedding intensity distributions.
* **Bar graphs** â€“ represent feature magnitudes.
* **Histograms** â€“ represent the frequency distribution of embedding values using the AlgX3 transformation method.

### Class-wise Organization

Images are stored in class-specific folders:

```
Heatmaps_merged_tensors_with_labels/
â”œâ”€â”€ 0/  (non-depressed class)
â””â”€â”€ 1/  (depressed class)
```

This structure enables direct loading using standard image-based deep learning pipelines.

---

## ğŸ§  Model Categories

### 1. Traditional Models

Located in: `traditional_vs_hybrid_models/traditional_text_models.ipynb`

* LSTM
* Support Vector Machine (SVM)
* Random Forest (RF)
* 1D CNN

These models operate directly on textual or vectorized features.

---

### 2. Hybrid Models

Located in: `traditional_vs_hybrid_models/bert_hybrid_models.ipynb`

* BERT + LSTM
* BERT + SVM
* BERT + RF
* BERT + 1D CNN

These models combine semantic embeddings with classical classifiers.

---

### 3. Deep Learning Models

Located in: `deep_learning_models/`

* **bert_only.ipynb** â€“ Pure BERT-based classification
* **bert_tweets.ipynb** â€“ BERT-based classification on Twitter dataset
* **BERT_v_Autoencoder.ipynb** â€“ Comparison of BERT and autoencoder representations
* **embedding_to_image_mapping.ipynb** â€“ Converts embedding vectors into images
* **GSGD_CNN.ipynb** â€“ 2D CNN optimized using Guided Stochastic Gradient Descent (GSGD)
* **ViT.ipynb** â€“ Vision Transformer-based classification
* **2dcnn_tweets.ipynb** â€“ CNN-based classification on Twitter-derived images

---

## âš™ï¸ GSGD Optimization Parameters

Guided Stochastic Gradient Descent (GSGD) is used for optimizing 2D CNN training:

**Major Parameters:**

* `lr` â€“ Learning rate
* `rho` â€“ Neighborhood size for batch consistency
* `batch_size` â€“ Training batch size

**Minor Parameters:**

* `revisit_batch_num` â€“ Number of revisited consistent batches
* `verification_set_num` â€“ Validation set for batch consistency

---

## ğŸ“¦ Requirements

* Python 3.x
* TensorFlow
* Keras
* NumPy
* Pandas
* Matplotlib
* Seaborn
* Scikit-learn

Install using:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage Workflow

### Step 1: Vector-to-Image Transformation

Run:

```
deep_learning_models/embedding_to_image_mapping.ipynb
```

This converts BERT vectors into heatmaps, bar graphs, and histogram images and saves them under `data/`.

---

### Step 2: Traditional Models

Run:

```
traditional_vs_hybrid_models/traditional_text_models.ipynb
```

---

### Step 3: Hybrid Models

Run:

```
traditional_vs_hybrid_models/bert_hybrid_models.ipynb
```

---

### Step 4: Deep Learning Models

Run any of the following:

* `GSGD_CNN.ipynb`
* `ViT.ipynb`
* `bert_only.ipynb`
* `BERT_v_Autoencoder.ipynb`
* `2dcnn_tweets.ipynb`

---

## â˜ï¸ Google Colab Support

1. Upload required CSV files to Colab.
2. Open the desired `.ipynb` file.
3. Install dependencies:

```bash
!pip install -r requirements.txt
```

4. Execute cells sequentially.

---

## ğŸ“œ License & Copyright

Copyright Â© 2026
**ECOLS Research Group â€“ All Rights Reserved**

---

## ğŸ™ Acknowledgments

This work builds upon advances in:

* Natural Language Processing (NLP)
* Deep Learning
* Vision-based representation learning

---

## ğŸ‘¥ Contributors

* **Anuraag Raj** â€“ Programmer & Research Developer
* **Dr. Anuraganand Sharma** â€“ Supervisor & Project Manager

---

âœ¨ This repository accompanies a published IEEE Access article and serves as a reproducible research framework for depression detection from social media.

ğŸ”— Official publication available at:
https://ieeexplore.ieee.org/document/11366856

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text==2.8.*\"\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "DQS0z8oTwEsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SPzcqKRY3b5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "\n"
      ],
      "metadata": {
        "id": "bMr2pBI13Ng7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/depression_dataset_reddit_mini.xlsx\")\n",
        "#df = pd.ExcelFile(\"/content/depression_dataset_reddit_mini.xlsx\")\n",
        "file_path = \"/content/data_split_8.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD9f01xELCVu",
        "outputId": "2168fba3-3279-457b-ce6d-063028569aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text  is_depression\n",
            "0  acummings i ve got return ticket booked for th...              0\n",
            "1  laying in bed and contemplating the meaning of...              0\n",
            "2  something s wrong i keep getting some error gr...              0\n",
            "3          not good munchkinster is not feeling well              0\n",
            "4  carobode i m starting to have an headache too ...              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use with splitting data!!!\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "#Separate into respective label outputs\n",
        "df_depressed = df[df['is_depression']=='1']\n",
        "df_not_depressed = df[df['is_depression']=='0']\n",
        "\n",
        "#Save separated data to on df\n",
        "#df_balanced = pd.concat([df_depressed, df_not_depressed])\n",
        "\n",
        "#Add label column and convert stored labels to binary values\n",
        "#df_balanced['is_depressed']=df_balanced['class'].apply(lambda x: 1 if x=='suicide' else 0)\n",
        "\n",
        "#Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'],df['is_depression'], stratify=df['is_depression'])"
      ],
      "metadata": {
        "id": "EmGrRrZy5-6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "hsyIf6Ok7C8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder\n",
        "#BERT\n",
        "#Callable BERT layers\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess(sentences)\n",
        "    return bert_encoder(preprocessed_text)['pooled_output']"
      ],
      "metadata": {
        "id": "PT-BPK-Q7H7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the 'is_depression' column\n",
        "df = df.drop('is_depression', axis=1)\n",
        "#convert the DataFrame to a Pandas Series\n",
        "series_from_df = df.squeeze()\n",
        "print(type(series_from_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QugNeiMwGxRH",
        "outputId": "632ee6c6-6b1e-42ea-a06c-dec96ed30562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encode\n",
        "#change df name as required\n",
        "#vals = get_sentence_embeding(X_train)\n",
        "vals = get_sentence_embeding(series_from_df)"
      ],
      "metadata": {
        "id": "wvo77ObT7SA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vals))"
      ],
      "metadata": {
        "id": "nXPsmqxU7b5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#convert EagerTensor to np array\n",
        "numpy_array = vals.numpy()\n",
        "\n",
        "#convert np array to pd df\n",
        "df = pd.DataFrame(numpy_array)\n",
        "\n",
        "csv_file_path = '/content/tensors_data_split_8.csv'\n",
        "\n",
        "#save the df to CSV\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Values saved to {csv_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LTkqTO75oB",
        "outputId": "7f6b3c31-f833-4dbc-cced-e1f106c9947b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values saved to /content/tensors_data_split_8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "\n",
        "#Separate into respective label outputs\n",
        "df_depressed = df[df['class']=='depressed']\n",
        "df_not_depressed = df[df['class']=='not_depressed']\n",
        "\n",
        "#Save separated data to on df\n",
        "df_balanced = pd.concat([df_depressed, df_not_depressed])\n",
        "\n",
        "#Add label column and convert stored labels to binary values\n",
        "#df_balanced['is_depressed']=df_balanced['class'].apply(lambda x: 1 if x=='suicide' else 0)\n",
        "\n",
        "#Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'],df['is_depressed'], stratify=df['is_depressed'])\n",
        "\n",
        "#BERT\n",
        "#Callable BERT layers\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess(sentences)\n",
        "    return bert_encoder(preprocessed_text)['pooled_output']"
      ],
      "metadata": {
        "id": "DsZS5xb23lF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5QmRg7Ql3AJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Vq6Xc11_vg1i",
        "outputId": "6c7bb7da-15fc-4f5b-c708-66ead9288bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\\npreprocessed_text = bert_preprocess(text_input)\\noutputs = bert_encoder(preprocessed_text)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "#df = pd.read_csv(\"/content/Suicide_Detection.xls\")\n",
        "\"\"\"xls = pd.ExcelFile(\"/content/Suicide_Detection_mini.xls\")\n",
        "df = pd.read_excel(xls, 'Suicide_Detection')\"\"\"\n",
        "df = pd.read_csv(\"/content/Suicide_Detection_mini.csv\")\n",
        "\n",
        "\n",
        "#Separate into respective label outputs\n",
        "df_suicide = df[df['class']=='suicide']\n",
        "df_non_suicide = df[df['class']=='non-suicide']\n",
        "\n",
        "#Save separated data to on df\n",
        "df_balanced = pd.concat([df_non_suicide, df_suicide])\n",
        "\n",
        "#Add label column and convert stored labels to binary values\n",
        "df_balanced['suicide']=df_balanced['class'].apply(lambda x: 1 if x=='suicide' else 0)\n",
        "\n",
        "#Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_balanced['text'],df_balanced['suicide'], stratify=df_balanced['suicide'])\n",
        "\n",
        "#BERT\n",
        "#Callable BERT layers\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess(sentences)\n",
        "    return bert_encoder(preprocessed_text)['pooled_output']\n",
        "\n",
        "#Model Build\n",
        "#Bert layers\n",
        "\"\"\"text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "preprocessed_text = bert_preprocess(text_input)\n",
        "outputs = bert_encoder(preprocessed_text)\"\"\"\n",
        "\n",
        "#print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vals = get_sentence_embeding(X_train)"
      ],
      "metadata": {
        "id": "1JPy4fDRwOZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP0axIztwO_d",
        "outputId": "2b6b403a-f794-4bdf-83f9-d2c75bf348c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.90716267 -0.6697711  -0.9813047  ... -0.88330615 -0.796734\n",
            "   0.9002798 ]\n",
            " [-0.8403879  -0.21145555 -0.4519796  ...  0.13402936 -0.55319035\n",
            "   0.8447843 ]\n",
            " [-0.66739255 -0.47472888 -0.87673515 ... -0.6522184  -0.5810513\n",
            "   0.7431891 ]\n",
            " ...\n",
            " [-0.64310694 -0.5759186  -0.93307877 ... -0.74471635 -0.65542066\n",
            "   0.6054077 ]\n",
            " [-0.84072965 -0.5289129  -0.9428792  ... -0.8568272  -0.6877374\n",
            "   0.856847  ]\n",
            " [-0.78556615 -0.49973592 -0.9749888  ... -0.90851486 -0.6878658\n",
            "   0.8301176 ]], shape=(14, 768), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nbQzos_3JbGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "i32y9oeqJdER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.optimizers import SGD\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils as utils\n",
        "from keras.layers import Dropout, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "3XduDrKFJgmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "(X, y), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "gAYkBbcXJlmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize data\n",
        "X, X_test = X.astype('float32')/255.0, X_test.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "MQBUQ-NvJlzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to categorical\n",
        "y, y_test = utils.to_categorical(y, 10), utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "yOyZuEj-Jl7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize model\n",
        "model = Sequential()"
      ],
      "metadata": {
        "id": "2VI_EK3yJmDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add conv 2d layer\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu'))\n",
        "#add dropout\n",
        "model.add(Dropout(0.01))\n",
        "#add conv 2d layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='valid'))\n",
        "#add max pool layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#flatten data\n",
        "model.add(Flatten())\n",
        "#add dense layer for cat\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "#add dropout\n",
        "model.add(Dropout(0.3))\n",
        "#add dense layer for cat\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(momentum=0.5, decay=0.0004), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R5-QPlUQJuhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model for 25 epochs\n",
        "model.fit(X, y, validation_data=(X_test, y_test), epochs=5, batch_size=512)"
      ],
      "metadata": {
        "id": "2KKrm9d7JuoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print accuracy\n",
        "print(\"Accuracy: \", (model.evaluate(X_test, y_test)[1]*100)) #&2.f%%"
      ],
      "metadata": {
        "id": "XKFmwA_OJuu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "}\n"
      ],
      "metadata": {
        "id": "du_FUVYdCtH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the original xlsx file path\n",
        "original_file_path = '/content/depression_dataset_reddit_cleaned.csv'\n",
        "\n",
        "# Read the original xlsx file into a DataFrame\n",
        "original_df = pd.read_csv(original_file_path)\n",
        "\n",
        "# Number of rows in each separated file\n",
        "chunk_size = 1000\n",
        "\n",
        "# Calculate the number of chunks needed\n",
        "num_chunks = (len(original_df) + chunk_size - 1) // chunk_size\n",
        "\n",
        "# Split the original DataFrame into chunks\n",
        "df_chunks = [original_df[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks)]\n",
        "\n",
        "# Save each chunk into a separate xlsx file\n",
        "for i, chunk in enumerate(df_chunks):\n",
        "    output_file_path = f'/content/data_split_{i+1}.csv'\n",
        "    chunk.to_csv(output_file_path, index=False)\n",
        "    print(f\"Chunk {i+1} saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75H73FfzCoiF",
        "outputId": "93d78e6d-2323-43f0-950e-497b1b7f45b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved to /content/data_split_1.csv\n",
            "Chunk 2 saved to /content/data_split_2.csv\n",
            "Chunk 3 saved to /content/data_split_3.csv\n",
            "Chunk 4 saved to /content/data_split_4.csv\n",
            "Chunk 5 saved to /content/data_split_5.csv\n",
            "Chunk 6 saved to /content/data_split_6.csv\n",
            "Chunk 7 saved to /content/data_split_7.csv\n",
            "Chunk 8 saved to /content/data_split_8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of file paths\n",
        "file_paths = [\n",
        "    '/content/tensors_data_split_1.csv',\n",
        "    '/content/tensors_data_split_2.csv',\n",
        "    '/content/tensors_data_split_3.csv',\n",
        "    '/content/tensors_data_split_4.csv',\n",
        "    '/content/tensors_data_split_5.csv',\n",
        "    '/content/tensors_data_split_6.csv',\n",
        "    '/content/tensors_data_split_7.csv',\n",
        "    '/content/tensors_data_split_8.csv'\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store individual DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append the DataFrame to the list\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate the DataFrames to merge them\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(\"Merged DataFrame:\")\n",
        "print(merged_df)\n",
        "\n",
        "#Read another file with the \"is_depression\" column\n",
        "dataset_file_path = '/content/depression_dataset_reddit_cleaned.csv'\n",
        "dataset_df = pd.read_csv(dataset_file_path)\n",
        "\n",
        "# Add the \"is_depression\" column to the merged DataFrame\n",
        "merged_df['is_depression'] = dataset_df['is_depression']\n",
        "\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "output_csv_path = '/content/merged_tensors_with_labels.csv'\n",
        "merged_df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoxFapW7f5xy",
        "outputId": "dbcc3544-de26-45e0-e9eb-e2f1429efbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged DataFrame:\n",
            "             0         1         2         3         4         5         6  \\\n",
            "0    -0.788955 -0.452171 -0.718902  0.524362  0.407345 -0.054561  0.599061   \n",
            "1    -0.786391 -0.536763 -0.928366  0.635211  0.677996 -0.164220  0.391323   \n",
            "2    -0.760979 -0.356124 -0.678704  0.650039  0.453287 -0.208375  0.471421   \n",
            "3    -0.645143 -0.442135 -0.855603  0.572795  0.629087 -0.262791  0.218573   \n",
            "4    -0.715979 -0.244338 -0.451217  0.467052  0.265069 -0.223246 -0.013612   \n",
            "...        ...       ...       ...       ...       ...       ...       ...   \n",
            "7726 -0.850394 -0.167222  0.098436  0.662542 -0.106068 -0.076882  0.842360   \n",
            "7727 -0.809725 -0.335600  0.215796  0.562724 -0.426850 -0.136305  0.844760   \n",
            "7728 -0.718596 -0.176832  0.043120  0.364462 -0.253737 -0.059141  0.647566   \n",
            "7729 -0.774526 -0.232257  0.028812  0.460020  0.081067 -0.086389  0.709499   \n",
            "7730 -0.808292 -0.308143 -0.763666  0.677819  0.504145 -0.133128  0.700517   \n",
            "\n",
            "             7         8         9  ...       758       759       760  \\\n",
            "0     0.322636 -0.314898 -0.999944  ...  0.471373  0.029673  0.846054   \n",
            "1     0.416578 -0.727519 -0.999947  ...  0.531579 -0.129403  0.900940   \n",
            "2     0.159647 -0.395316 -0.999743  ...  0.255220  0.439547  0.853939   \n",
            "3     0.249878 -0.622201 -0.999599  ...  0.320956  0.541194  0.944661   \n",
            "4     0.125522 -0.200415 -0.999057  ...  0.247937  0.191682  0.713078   \n",
            "...        ...       ...       ...  ...       ...       ...       ...   \n",
            "7726  0.120478  0.050714 -0.999742  ...  0.435052  0.310392  0.247697   \n",
            "7727  0.229188  0.132159 -0.999725  ...  0.328730  0.160175  0.160973   \n",
            "7728  0.144380  0.100780 -0.999658  ...  0.395835  0.618902  0.194434   \n",
            "7729  0.111651 -0.094884 -0.999622  ...  0.422958  0.285806  0.341141   \n",
            "7730  0.192945 -0.570813 -0.999923  ...  0.278250 -0.401789  0.958738   \n",
            "\n",
            "           761       762       763       764       765       766       767  \n",
            "0     0.820441  0.263943  0.345332  0.482984 -0.372246 -0.635277  0.699350  \n",
            "1     0.696012  0.549645 -0.077668  0.599802 -0.796655 -0.683269  0.622754  \n",
            "2     0.784838  0.883011  0.568982  0.452909 -0.571980 -0.646512  0.888765  \n",
            "3     0.662793  0.681176  0.400102  0.513169 -0.658904 -0.574159  0.832582  \n",
            "4     0.695555  0.938667  0.679499  0.416515 -0.203758 -0.548361  0.812973  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  \n",
            "7726  0.808360  0.110372  0.554971  0.481810  0.174321 -0.543799  0.876660  \n",
            "7727  0.840339  0.637517  0.635649  0.355673  0.382026 -0.559449  0.851121  \n",
            "7728  0.668961  0.387896  0.390159  0.487443  0.497392 -0.477524  0.776006  \n",
            "7729  0.756394  0.272730  0.615420  0.465264 -0.064373 -0.506221  0.860842  \n",
            "7730  0.788129  0.177132  0.476603  0.368799 -0.685175 -0.611195  0.869874  \n",
            "\n",
            "[7731 rows x 768 columns]\n"
          ]
        }
      ]
    }
  ]
}